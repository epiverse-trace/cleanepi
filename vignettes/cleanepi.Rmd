---
title: "cleanepi"
output: 
  rmarkdown::html_vignette:
    df_print: "kable"
vignette: >
  %\VignetteIndexEntry{cleanepi}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE} 
knitr::opts_chunk[["set"]](collapse = TRUE, comment = "#>", eval = FALSE,
                           fig.width = 7L, fig.height = 7L,
                           fig.align = "center")
row_id <- group_id <- NULL
```


## An overview

No matter how many measures are taken, one must always expect messy data from the real-world, with duplicates, errors, incomplete, or irrelevant formats. Data cleaning is an essential step in data analysis. It allows to produce accurate, reliable and reproducible results. However, data cleaning is a significant barrier in data analysis because it takes a long time to complete.

**{cleanepi}** is an R package to clean, curate, and standardize epidemiological data. It contains functions to perform several data cleaning tasks that an end-user would anticipate to be performed on a cluttered dataset.

**{cleanepi}** is specifically designed for epidemiological data, and works with data frame-like data structure. This vignette provides a detailed description of the functions included in the package and how to use them.

```{r setup, eval=TRUE}
library("cleanepi")
library("magrittr")
```

## General data cleaning tasks

The main function in **{cleanepi}** is `clean_data()` that can perform the following tasks:

1. Scan the input data to determine the percent of missing, numeric, character, logical and date values in every column of the input data frame.   
2. Clean up column names and convert them to more sensible formats. This includes many sub-tasks such as changing a space, dot, or hyphen between two words with underscore; converting camel-cases to snake-cases; substituting foreign characters with their corresponding English characters; and splitting a long word into multiple short words by capital characters within, if any, and connecting them with underscores.   
3. Remove duplicated rows (across all columns or some specific columns). This also includes the removal of empty rows and columns as well as constant columns, i.e. columns with the same value across all rows.   
4. Replace missing entries with `NA`. 
5. Check whether the the sequence of date events are correct in all rows of the input data.
6. Convert `character` columns into `Date` if the column actually contains values of type `Date` to some extent (default is 50% of the values are `Date`).
7. Detect and remove rows with subject IDs that do not comply with the expected format.
8. Perform dictionary-based cleaning: replace keys in specific columns with their corresponding values stored in a data dictionary file, replace misspelled values with their correct ones.
9. Convert numbers written in characters into numeric.

Every cleaning operation in **{cleanepi}** is implemented as a module. These modules are described in details in the package design vignette.

```{r eval=TRUE, comment="read_input"}
# IMPORTING THE TEST DATASET
test_data       <- readRDS(system.file("extdata", "test_df.RDS",
                                       package = "cleanepi"))
head(test_data)

# IMPORTING THE DATA DICTIONARY
test_dictionary <- readRDS(system.file("extdata", "test_dictionary.RDS",
                                       package = "cleanepi"))
head(test_dictionary)
```

```{r eval=TRUE, comment="scan_data"}
# SCAN THE DATA
scan_result     <- scan_data(test_data)
scan_result
```

```{r eval=TRUE}
# DEFINE INPUT FOR EACH MODULE
# OPERATIONS FOR REPLACING MISSING VALUES
rm_na                  <- list(target_columns = NULL, na_strings = "-99")
# OPERATIONS FOR DUBLICATES AND CONSTANTS COLUMNS AND ROWS
rm_dup               <- list(target_columns   = NULL, # to consider duplicate across all columns
                                rm_empty_rows    = TRUE, #
                                rm_empty_cols    = TRUE,
                                rm_constant_cols = TRUE)
# OPERATIONS FOR STANDARDING DATES
stdn_date        <- list(target_columns  = NULL,
                                error_tolerance = 0.5,
                                format          = NULL,
                                timeframe       = as.Date(c("1973-05-29",
                                                            "2023-05-29")))
#  OPERATIONS FOR STANDARDING subject ID
stdn_ids <- list(id_col_name = "study_id", # the target column
                                format      = NULL,
                                prefix      = "PS",
                                suffix      = "P2",
                                range       = c(1, 100))
# LAOD A DATA DICTIONARY
test_dictionary <- readRDS(system.file("extdata", "test_dictionary.RDS",
                                       package = "cleanepi"))
params <- list(
  replace_missing_values  = rm_na, 
  remove_duplicates       = rm_dup,
  standardize_date        = stdn_date,
  standardize_subject_ids = stdn_ids,
  dictionary              = test_dictionary
)
```

```{r eval=TRUE}
# CLEAN THE INPUT DATA FRAME
res <- clean_data(
  data   = test_data,
  params = params
)
```

The function `clean_data()` returns a `list` of `r length(res)` objects:

1. **`r names(res)[1]`**: a `data.frame` or `linelist` object with the cleaned data.
2. **`r names(res)[2]`**: a `list` of reports from all the actions performed on the given dataset.

```{r eval=TRUE}
# PRINT TOP 6 ROWS OF THE CLEANED DATA
head(res[["data"]])

# SUMMARIZE THE REPORT OBJECT
summary(res[["report"]])
```

**Note:** a function to visualize this report object will be built in the next release of the package.

## Specific data cleaning tasks
Certain data cleaning operations are automatically applied to input data. These operations include renaming columns, removing empty rows and columns, removing columns with the same values across all rows, and standardizing date columns. We refer to these as the `implicit data cleaning steps`,  which are executed by default.

However, we also provide users with the flexibility to call specific functions for these data cleaning steps separately if they wish to perform a particular task individually. This approach allows users to have more control over the data cleaning process and to apply additional data cleaning functions as needed.

This setup offers users both convenience and flexibility, as they can benefit from default data cleaning operations while also having the option to customize their data cleaning process according to their specific requirements.


### Standardizing Dates

The `standardize_date()` function provides a comprehensive set of options for converting date columns into a specified format and handling various scenarios, such as different date formats and mixed data types in a column. Below we summarize the key parameters of the function:

The default date format in R is `%Y-%m-%d`. However, it is very common to encounter date values recoded differently from this. Also, there are cases where a column in a data frame contains both values of type `Date`, `character` or others.    
The `standardize_date()` function offers the possibility to convert date columns into `%Y-%m-%d` format and convert `character` columns into `Date` if the percentage of date values reach a specified threshold. The function needs the following arguments:

1. `data`: A data frame or linelist  (required)   
2. `date_column_name`: A vector of the names of the columns to be converted (required)   
3. `format`: A format of the values in the specified columns (optional). If not provided, the function will attempt to infer the format
4. `timeframe`: The expected time frame within which the date values should fall. Values outside this range will be set to `NA`(optional).
5. `error_tolerance`: The minimum percentage of values of type Date in a character column needed to convert it into a Date column. Default is 50% i.e. `0.5`.    


⚠️ The `error_tolerance` must be used with caution. When it is set, and the  percentage of date values in a character column is less than this threshold, the column will be returned as it is. 

This function provides users with the flexibility to standardize date columns in their dataset according to specified requirements, including format, timeframe, and error tolerance for conversion from character to date columns.


```{r eval=TRUE}
res <- standardize_dates(
  data             = readRDS(system.file("extdata", "test_df.RDS",
                                         package = "cleanepi")),
  target_columns  = "date_first_pcr_positive_test",
  format           = NULL,
  timeframe        = NULL,
  error_tolerance  = 0.5
)
```

This function returns a `list` of `r length(res)` elements named as:

* **`r names(res)[1]`**: the input dataset where the (specified) columns are converted into date if the condition is met.
* **`r names(res)[2]`**: a `list`, or augmented  report `list` with an element named as **`r names(res$report)`** that contains a `vector` of the transformed column names.    

### Standardizing subject IDs

The `check_subject_ids()` function is designed to identify and eliminate rows from the input dataset that don't conform to the expected format for subject IDs. To implement this function, you need to provide the following parameters:

1. **`data`** (required): This parameter represents the input dataset.

2. **`id_column_name`** (required): Specify the name of the column containing the subject IDs in the dataset.

3. **`format`** (required): Define the expected format for the subject IDs. The function will use this format to validate the IDs.

4. **`prefix`** (optional): If subject IDs have a specified prefix, provide it as an argument. This is optional and can be omitted if there is no prefix.

5. **`suffix`** (optional): Similarly, if subject IDs have a specified suffix, include it here. If there is no suffix, you can skip this parameter.

6. **`range`** (optional): If there is an expected range of numbers within the subject IDs, define it using this parameter. It is optional and can be omitted if there is no specific range.

By incorporating these parameters, the function becomes a versatile tool for data cleaning, ensuring that only rows with subject IDs adhering to the specified format are retained in the dataset. When using the function, make sure to tailor the parameters according to the specific requirements of your dataset and the expected characteristics of the subject IDs.

```{r eval=TRUE}
# DETECT INCORRECT SUBJECT IDs
res <- check_subject_ids(
  data           = readRDS(system.file("extdata", "test_df.RDS",
                                       package = "cleanepi")),
  id_column_name = "study_id",
  format         = "PS000P2",
  prefix         = "PS",
  suffix         = "P2",
  range          = c(1L, 100L)
)
```

The `check_subject_ids()` function returns a list with elements named according to the following conventions:

1. **`r names(res)[1]`**: This element contains either the original input dataset or a subset of it where rows with incorrect IDs have been removed, depending on whether the `remove` parameter is set to `TRUE`.

2. **`r names(res)[2]`**: This element is a list, possibly an augmented report list, where one of its elements is named as **`r names(res$report)`**. This particular element contains a data frame detailing the rows that were detected to have incorrect subject IDs.

To summarize:

- Element 1: Contains the cleaned dataset, either the original or a subset with rows containing bad IDs removed.
- Element 2: Contains a list, possibly with additional information, and one of its elements provides a data frame specifying the rows with incorrect subject IDs.

This structure facilitates easy access to both the cleaned dataset and a detailed report on the detected issues with subject IDs, enhancing the transparency and usability of the function's output.

```{r eval=TRUE}
# DISPLAY ROWS WITH THE INCORRECT SUBJECT IDs
res[["report"]][["incorrect_subject_id"]]
```
### Calculating age in different time scales

The `calculate_age()` function computes the ages of individuals, expressed in years, months, weeks, or days, based on a given date column and a reference date. It requires the following arguments:

1. **`data`** (required): This parameter represents the input dataset containing the relevant information.
   
2. **`date_column_name`** (required): Specify the name of the column containing the birth or relevant dates for which the age is being calculated.
   
3. **`end_date`** (required): This is the reference date used to calculate the age of individuals. The function computes the age relative to this date.
   
4. **`age_in`** (optional): This parameter determines the unit in which the age is expressed. It can be specified as "years", "months", "weeks", or "days". By default, the age is calculated in years if this parameter is not provided.

By utilizing these arguments, the function offers flexibility in determining the age of individuals based on different units and reference dates. It facilitates various analytical tasks where age computation is a necessary component, providing users with the ability to customize the output according to their specific requirements.

```{r eval=TRUE}
# CALCULATE INDIVIDUAL AGE FROM THE `dateOfBirth` COLUMN
age <- calculate_age(
  data          = readRDS(system.file("extdata", "test_df.RDS",
                                       package = "cleanepi")),
  target_column = "dateOfBirth", 
  age_in        = "days", 
  na_strings    = "-99"
)
```

The `calculate_age()` function is designed to enhance the input dataset by adding one or two extra columns containing pertinent age-related information. These additional columns are as follows:

1. **Calculated Age in Specified Unit**: This column contains the calculated age of individuals expressed in the specified unit (years, months, weeks, or days).

2. **Remaining Number of Days**: This optional column indicates the remaining number of days after calculating the age, representing the fractional part of the age calculation. This column is included if needed, providing additional granularity in age representation.

```{r eval=TRUE}
# DISPLAY THE OUTPUT OBJECT
head(age)  # note the last 2 columns
```

### Checking date sequence
The `check_date_sequence()` function verifies the order of sequences in date event columns within a dataset. It ensures that the values in specified date columns follow the desired chronological order. Here are the arguments accepted by the function:

1. **`data`**: This parameter represents the input dataset that is being analyzed for the correctness of date sequences.
2. **`target_columns`**: A vector containing the names of date columns of interest. These columns should be listed in the expected order of occurrence, reflecting the chronological sequence of events. For example, `target_columns = c("date_of_infection", "date_of_admission", "date_of_death")`.
3. **`remove_bad_seq`**: A Boolean variable with a default value of `FALSE`. If set to `TRUE`, rows with incorrect date sequences will be removed from the output object. Otherwise, they will be flagged as erroneous and stored in the report object.

By utilizing these arguments, the `check_date_sequence()` function facilitates the validation of date sequences within a dataset, ensuring data integrity and accuracy for further analysis. Additionally, it offers flexibility by allowing users to choose whether to remove rows with incorrect sequences or store them for further examination in the report object.

```{r eval=TRUE}
# DETECT ROWS WITH INCORRECT DATE SEQUENCE
res <- check_date_sequence(
  data           = readRDS(system.file("extdata", "test_df.RDS",
                                       package = "cleanepi")),
  target_columns     = c("date_first_pcr_positive_test", "date.of.admission"),
  remove_bad_seq = FALSE
)
```

```{r eval=TRUE, include=FALSE}
target_name <- names(res[["report"]])
```

The `check_date_sequence()` function returns a list with elements named according to the following conventions:

1. **`r names(res)[1]`**: This element contains either the original input dataset or a subset of it without the rows that have incorrect date sequences, depending on whether the `remove_bad_seq` parameter is set to `TRUE`.
2. **`r names(res)[2]`**: This element is a list, possibly an augmented report list, with an element named as **`r names(res$report)`**. This sub-list, in turn, consists of two elements:

   - **`r names(res$report[[target_name]])[1]`**: This element provides information about the expected order of the date events, as specified by the `event_cols` parameter.
   
   - **`r names(res$report[[target_name]])[2]`**: This element contains a data frame listing the rows with incorrect date sequences detected during the validation process.

By organizing the output in this manner, the function offers clarity and structure, making it easier for users to access and interpret the results of the date sequence validation. It provides both the cleaned dataset and a detailed report highlighting any discrepancies found in the date sequences, enabling users to take appropriate actions based on their specific requirements and analysis goals. 

```{r eval=TRUE}
# PRINT ROWS WITH INCORRECT DATE SEQUENCE
res[["report"]]
```

### Finding duplicated rows

The `find_duplicates()` function serves the purpose of identifying duplicated rows within a given dataset. It accepts the following parameters:

1. `data`: A dataframe on which the duplication check will be performed.
   
2. `target_columns`: A vector containing either column names or indexes from which duplicated rows will be identified. If `NULL` is passed, duplicates will be detected across all columns of the dataset. Notably, if the input dataset is a `linelist` object, `target_columns` can be set to `tags` specifically to identify duplicates across the tagged variables only. 

By leveraging the `find_duplicates()` function with appropriate parameters, users can efficiently pinpoint duplicated rows within their datasets, either across all columns or selectively across tagged variables in a `linelist` object.

```{r eval=TRUE}
# IMPORT A `linelist` DATA
data <- readRDS(system.file("extdata", "test_linelist.RDS",
                            package = "cleanepi"))
head(data)
```

```{r eval=TRUE}
# SHOW THE TAGGED VARIABLES
linelist::tags(data)
```

```{r eval=TRUE}
# FIND DUPLICATES ACROSS TAGGED VARIABLES
dups <- find_duplicates(
                        data           = data,
                        target_columns = "tags"
)
```

Upon execution, the `find_duplicates()` function identifies and returns all duplicated rows within the dataset, either based on all columns or those specified. Additionally, it appends two extra columns to the dataset:

1. `row_id`: This column indicates the row numbers of the duplicated rows from the original input dataset.

2. `group_id`: Each duplicated group is assigned a unique identifier in this column. A duplicated group comprises rows with identical values in the specified columns of interest.

By including these extra columns, users gain insights into the specific rows identified as duplicates and their corresponding group identifiers, enabling efficient analysis and management of duplicated data within the dataset.

```{r eval=TRUE}
# VISUALIZE THE DUPLICATES
head(dups %>% dplyr::select(c(row_id, group_id,
                              as.character(linelist::tags(data)))))
```

### Removing duplicates

To eliminate duplicated rows from a dataset, the `remove_duplicates()` function can be employed. This function internally utilizes the `find_duplicates()` function and expects the following parameters:

1. `data`: This parameter represents the input dataset from which duplicated rows will be removed.
2. `target_columns`: It is a vector containing either column names or indexes specifying the columns from which duplicated rows will be identified. If set to `NULL`, the function will detect duplicates across all columns. If the input dataset is a `linelist` object, setting this parameter to `tags` will identify duplicates across the tagged variables only.
3. `remove`: This is a numeric vector comprising the indices of the duplicated rows to be removed. If set to `NULL`, the function removes duplicates and retains only the first occurrence of the duplicated rows.
4. `rm_empty_rows`: A Boolean to indicate whether to remove empty rows or not.
5. `rm_empty_cols`: A Boolean to indicate whether to remove empty columns or not.
6. `rm_constant_cols`: A Boolean to indicate whether to remove constant columns or not.


```{r eval=TRUE}
# IMPORT  DATA
#CALLING THE REMOVE_DUPLICATE FUNCTION.
res <- remove_duplicates(
  data = readRDS(system.file("extdata", "test_linelist.RDS",
                            package = "cleanepi")),
  target_columns = NULL,
  remove         = NULL
)
```

Upon execution, the `remove_duplicates()` function returns two main outputs:

1. The input dataset with duplicated rows removed.
2. A list containing details about the duplicates removal operation, labeled `res$report`.

The `res$report` list consists of elements with the following information:

- **`names(res$report$remove_duplicates)[1]`**: This subset of the input dataset contains all the duplicated rows detected using the `find_duplicates()` function.

- **`names(res$report$remove_duplicates)[2]`**: This subset of the input dataset represents the rows that were removed due to duplication.

- **`names(res$report$remove_duplicates)[3]`**: This is a comma-separated string containing the column names that were used to identify the duplicated rows.

By examining these elements within the `res$report` list, users gain insights into the specific duplicated rows detected, those that were removed, and the columns used to identify the duplications, thus facilitating transparency and documentation of the duplicates removal process.

```{r eval=TRUE}
# STORE THE DUPLICATES REMOVAL PROCESS REPORT IN A VARIABLE
#dups_removal_report <- res[["report"]][["remove_duplicates"]]

# DISPLAY ALL DUPLICATES FOUND
#head(dups_removal_report[["all_dups"]] %>%
 #      dplyr::select(c(row_id, group_id, as.character(linelist::tags(data)))))

# DISPLAY THE DUPLICATED LINES THAT WAS REMOVED
#head(dups_removal_report[["removed_dups"]] %>%
 #      dplyr::select(c(row_id, group_id, as.character(linelist::tags(data)))))

# DISPLAY COLUMN NAMES USED TO DETECT DUPLICATES
#dups_removal_report[["duplicates_checked_from"]]
```

The output from  `find_duplicates()` function can also be passed to `remove_duplicates()` function to specify which duplicated rows to be removed.  

```{r eval=TRUE}
# DETECT DUPLICATES FROM TAGGED COLUMNS
dups <- find_duplicates(
  data           = readRDS(system.file("extdata", "test_linelist.RDS",
                                       package = "cleanepi")),
)

# REMOVE FIRST OCCURRENCE OF DUPLICATED ROWS
#dups_index_to_remove <- dups[["row_id"]][seq(1L, nrow(dups), 2L)]
no_dups <- remove_duplicates(
  data           = readRDS(system.file("extdata", "test_linelist.RDS",
                                       package = "cleanepi")),
  remove         = NULL#dups_index_to_remove
)
```

### Dictionary based data substituting

The `clean_using_dictionary()` function offers a convenient way to replace the options in a data frame or linelist with their corresponding values stored in a data dictionary file. The structure of this data dictionary file should adhere to the standards expected by the [matchmaker](https://www.repidemicsconsortium.org/matchmaker/) package, as the `clean_using_dictionary()` function relies on functions from this package.

Here's how the function operates:

- **Inputs**: The function accepts two main inputs:
  1. The input data frame or linelist that contains the options to be replaced.
  2. The corresponding data dictionary file that contains the complete labels for these options. This dictionary file typically includes additional information necessary for understanding the input dataset.

- **Data Dictionary Structure**: The structure of the data dictionary should align with the format expected by the matchmaker package. This format ensures compatibility and seamless integration with the `clean_using_dictionary()` function.

By leveraging the `clean_using_dictionary()` function with a suitable data dictionary, users can effectively replace option values in their datasets with their corresponding labels, enhancing the interpretability and clarity of the data.

```{r eval=TRUE, comment="display_dictionary"}
test_dictionary
```

The `add_to_dictionary()` function is a useful tool for expanding the coverage of a data dictionary by defining options that are present in the input data but not originally included in the dictionary. This function enables users to dynamically update the dictionary to accommodate new values encountered in the dataset.

Here's how the `add_to_dictionary()` function can be utilized:

1. **Identify Undefined Options**: Users can identify options in the input data that are not defined in the existing data dictionary. For instance, in your example, the value `-99` is not defined in the dictionary.

2. **Call `add_to_dictionary()`**: Users can then use the `add_to_dictionary()` function to add the undefined options to the data dictionary. This function facilitates the expansion of the dictionary to include new values encountered in the dataset.

By employing the `add_to_dictionary()` function, users can ensure that the data dictionary remains comprehensive and aligned with the evolving nature of the input dataset, thereby enhancing the accuracy and completeness of data interpretation and analysis. 
In the example below, we add `-99` to our test data dictionary, `test_dictionary`.

```{r eval=TRUE}
## READING IN THE DATA
data <- readRDS(system.file("extdata", "test_df.RDS",
                            package = "cleanepi"))

## ADD THE EXTRA OPTION TO THE DICTIONARY
test_dictionary <- add_to_dictionnary(test_dictionary,
                                      option = "-99",
                                      value  = "unknow",
                                      grp    = "sex",
                                      order  = NULL)
test_dictionary
```


```{r eval=TRUE, comment="dictionary-base_cleaning"}
cleaned_df <- clean_using_dictionary(
  data       = data,
  dictionary = test_dictionary
)

cleaned_df
```




